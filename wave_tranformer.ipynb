{
 "cells": [
  {
   "source": [
    "# Wave classification using BERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# import tensorflow as tf\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#           gpus[0],\n",
    "#             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.signal import shape_ops\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "fold_index = 0\n",
    "task = 'home'\n",
    "model_type = 'attention'\n",
    "\n",
    "model_dir = './model'\n",
    "data_dir = './data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.experimental.set_memory_growth(\n",
    "#     tf.config.list_physical_devices('GPU')[0], allow_memory_growth\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "model_dir = Path(model_dir)\n",
    "data_dir = Path(data_dir)\n",
    "model_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cde081e8fee24f0a86fd087d50d950ac"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Load training setup\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from libs.misc import wavio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "root_dir = data_dir / 'evaluation_setup'\n",
    "\n",
    "data_cache = {}\n",
    "\n",
    "def get_annotation(task, fold_index, target):\n",
    "    df = pd.read_csv(\n",
    "        root_dir / f'{task}_fold{fold_index+1}_{target}.txt', sep='\\t', \n",
    "        header=None, names=['file', 'class', 'start', 'end', 'event']\n",
    "    )\n",
    "    df['id'] = df['file'].apply(lambda x: Path(x).stem)\n",
    "    return df\n",
    "\n",
    "def load_dataset(target):\n",
    "    if target not in data_cache:\n",
    "        df = get_annotation(task, fold_index, target)\n",
    "        wav_dict = {}\n",
    "        for file in tqdm(df['file'].unique()):\n",
    "            wav_dict[Path(file).stem] = wavio.readwav(str(data_dir / file))\n",
    "        data_cache[target] = (df, wav_dict)\n",
    "        return df, wav_dict\n",
    "    else:\n",
    "        return data_cache[target]\n",
    "df, wav_dict = load_dataset('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['nan', '(object) rustling', '(object) snapping', 'cupboard', 'cutlery', 'dishes', 'drawer', 'glass jingling', 'object impact', 'people walking', 'washing dishes', 'water tap running']\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "sound_events = ['nan']\n",
    "\n",
    "for fold, task_name in product([0,1,2,3], ['train', 'evaluate', 'test']):\n",
    "    sound_events.extend(get_annotation('home', fold, task_name)['event'].unique())\n",
    "sound_events = list(filter(lambda x: x != 'nan', np.unique(sound_events)))\n",
    "sound_events = ['nan'] + sound_events\n",
    "print(sound_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "n_augmentation = 5\n",
    "perturbation = (0.0, 0.05)\n",
    "\n",
    "def parse_wave(series, wav_dict, start=0.0, end=5.0, duration=5.0):    \n",
    "    result = series.to_dict()\n",
    "    sr, bw, audio = wav_dict[series['id']]\n",
    "    \n",
    "    data = np.zeros((int(duration * sr), 2), dtype=np.float32)\n",
    "    mask = np.zeros((int(duration * sr), 1), dtype=np.bool)\n",
    "    \n",
    "    start = int(start * sr)\n",
    "    end = int(end * sr)\n",
    "    data[start:end] = audio[start:end]\n",
    "    mask[start:end] = True\n",
    "    \n",
    "    result['id'] = series['id']\n",
    "    result['start_index'] = max(start, 0)\n",
    "    result['end_index'] = min(end, len(audio))\n",
    "    result['sr'] = sr\n",
    "    result['bw'] = bw\n",
    "    result['audio'] = data\n",
    "    result['mask'] = mask\n",
    "    \n",
    "    return pd.Series(result)\n",
    "\n",
    "_id = df.loc[0, 'id']\n",
    "sr = wav_dict[_id][0]\n",
    "# display(Audio(wav_dict[_id][2][:, 0], rate=sr))\n",
    "# display(Audio(\n",
    "#     parse_wave(df.loc[0], wav_dict=wav_dict)['audio'][:, 0], rate=sr\n",
    "# ))\n",
    "# audio_df = df.apply(partial(parse_wave, wav_dict=wav_dict, window_size=5.0, hop_size=1.25), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import librosa\n",
    "import joblib\n",
    "import random \n",
    "\n",
    "n_mels = 128\n",
    "n_sampling=4096\n",
    "hop_length = n_sampling // 4\n",
    "window='hann'\n",
    "pad='constant'\n",
    "max_time = 0.5\n",
    "batch_size = 2\n",
    "\n",
    "def preprocess(wav, sampling_rate):\n",
    "    return np.concatenate([\n",
    "        mono_preprocess(wav[..., 0], sampling_rate)[..., np.newaxis],\n",
    "        mono_preprocess(wav[..., 1], sampling_rate)[..., np.newaxis],\n",
    "    ], axis=-1)\n",
    "\n",
    "def mono_preprocess(wav, sampling_rate):\n",
    "    mag = librosa.feature.melspectrogram(\n",
    "        wav, sr=sampling_rate, hop_length=hop_length, n_mels=n_mels,\n",
    "        fmin=0.0, fmax=20e3,\n",
    "    )\n",
    "    logmag = np.log(mag + 1e8)\n",
    "    return logmag\n",
    "\n",
    "def normalize_time(audio_df, max_time):\n",
    "    results = []\n",
    "    for audio, sr in zip(audio_df['audio'], audio_df['sr']):\n",
    "        max_len = int(sr * max_time)\n",
    "\n",
    "        pos = min(len(audio), max_len)\n",
    "        result = np.zeros((max_len, 2), np.float32)\n",
    "        result[:pos, :] = audio[:pos]\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_dataset(target):\n",
    "    df, wav_dict = load_dataset(target)\n",
    "    encoding_dict = {event: i for i, event in enumerate(sound_events)}\n",
    "    \n",
    "    def yield_wave():\n",
    "        def _parse_wave(series):\n",
    "            start = random.uniform(0.0, 0.1 * max_time)\n",
    "            end = random.uniform(0.1 * max_time, max_time)\n",
    "            end = end if (max_time - start) > (start + end) else (max_time - start)\n",
    "            return parse_wave(series, wav_dict, start=start, end=end, duration=max_time)\n",
    "        \n",
    "        def normalize(audio):\n",
    "            max_value = np.amax(audio)\n",
    "            min_value = np.amin(audio)\n",
    "            return (audio - min_value) / (max_value - min_value)\n",
    "        \n",
    "        for i in df.index:\n",
    "            series = _parse_wave(df.loc[i])\n",
    "            event = encoding_dict[series['event']]\n",
    "            indices = np.linspace(0, series['audio'].shape[0] // 16 * 16, int(max_time * 16000), dtype=np.int32)\n",
    "            yield normalize(series['audio'])[indices, :], series['mask'][indices, :], event\n",
    "    return yield_wave\n",
    "    \n",
    "def get_tf_dataset(target, batch_size=32, shuffle=False, **kwargs):\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        get_dataset(target, **kwargs),\n",
    "        output_types=(tf.float32, tf.bool, tf.int32)\n",
    "    )\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=shuffle)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_tf_dataset('train', batch_size=batch_size, shuffle=True)\n",
    "test_dataset = get_tf_dataset('evaluate', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[[0.5414052 , 0.5414052 ],\n",
       "         [0.5414052 , 0.5414052 ],\n",
       "         [0.5414052 , 0.5414052 ],\n",
       "         ...,\n",
       "         [0.5414052 , 0.5414052 ],\n",
       "         [0.5414052 , 0.5414052 ],\n",
       "         [0.5414052 , 0.5414052 ]],\n",
       " \n",
       "        [[0.42060146, 0.42060146],\n",
       "         [0.42060146, 0.42060146],\n",
       "         [0.42060146, 0.42060146],\n",
       "         ...,\n",
       "         [0.42060146, 0.42060146],\n",
       "         [0.42060146, 0.42060146],\n",
       "         [0.42060146, 0.42060146]]], dtype=float32),\n",
       " array([[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         ...,\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]]),\n",
       " array([4, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "next(train_dataset.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class PlotCallback(tf.keras.callbacks.Callback):\n",
    "    is_higher_better = {\n",
    "        'accuracy'\n",
    "    }\n",
    "    is_linear = {\n",
    "        'accuracy',\n",
    "        'sparse_categorical_accuracy',\n",
    "    }\n",
    "    def __init__(self, targets=None, n_step=1):\n",
    "        super().__init__()\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "        self.axes_index = {}\n",
    "        self.n_step = n_step\n",
    "        self.targets = targets\n",
    "        self.epochs = []\n",
    "        self.history = defaultdict(list)\n",
    "\n",
    "    def plot_and_display(self):\n",
    "        for ax in self.axes.flat:\n",
    "            ax.clear()\n",
    "        for i, (label, values) in enumerate(self.history.items()):\n",
    "            if any(name in label for name in self.is_higher_better):\n",
    "                get_best_value = np.amax\n",
    "            else:\n",
    "                get_best_value = np.amin\n",
    "            \n",
    "            if label.startswith('val_'):\n",
    "                _label = label[4:]\n",
    "            else:\n",
    "                _label = label\n",
    "            \n",
    "            ax = self.axes.flat[self.axes_index[_label]]\n",
    "            ax.plot(self.epochs, values, label=label, color=f'C{i}')\n",
    "            best_value = get_best_value(values)\n",
    "            ax.axhline(best_value, linestyle='--', color=f'C{i}')\n",
    "            ax.text(0.0, best_value, f'{best_value:.3f}')\n",
    "            \n",
    "            if _label not in self.is_linear:\n",
    "                ax.set_yscale('log')\n",
    "\n",
    "        if self.epochs[-1] == 0:\n",
    "            self.fig.legend()\n",
    "\n",
    "        io = BytesIO()\n",
    "        self.fig.savefig(io, format='png')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display_png(Image(io.getvalue()))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch == 0:\n",
    "            self.fig, self.axes = plt.subplots(len(logs) // 2, 1, figsize=(8, 4 * len(logs) // 2))\n",
    "            self.axes_index = {}\n",
    "            for label in logs:\n",
    "                if label.startswith('val_'):\n",
    "                    _label = label[4:]\n",
    "                else:\n",
    "                    _label = label\n",
    "                if _label not in self.axes_index:\n",
    "                    self.axes_index[_label] = len(self.axes_index)\n",
    "\n",
    "        for key, value in logs.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "        self.epochs.append(epoch)\n",
    "        if (epoch % self.n_step) == 0:\n",
    "            self.plot_and_display()\n",
    "\n",
    "class BalancedSparseCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n",
    "    def __init__(self, name='balanced_sparse_categorical_accuracy', dtype=None):\n",
    "        super().__init__(name, dtype=dtype)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_flat = y_true\n",
    "        if y_true.shape.ndims == y_pred.shape.ndims:\n",
    "            y_flat = tf.squeeze(y_flat, axis=[-1])\n",
    "        y_true_int = tf.cast(y_flat, tf.int32)\n",
    "\n",
    "        cls_counts = tf.math.bincount(y_true_int)\n",
    "        cls_counts = tf.math.reciprocal_no_nan(tf.cast(cls_counts, self.dtype))\n",
    "        weight = tf.gather(cls_counts, y_true_int)\n",
    "        return super().update_state(y_true, y_pred, sample_weight=weight)\n",
    "    \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bigbird.core.encoder import EncoderStack\n",
    "from bigbird.core import utils\n",
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "\n",
    "@dataclass\n",
    "class EncoderConfig:\n",
    "    hidden_size: int = 768\n",
    "    seq_size: int = 4096\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_mask_prob: float = 0.0 # Dropout probability for training\n",
    "    num_attention_heads: int = 12\n",
    "    num_hidden_layers: int = 8\n",
    "\n",
    "    embedding_kernel:int = None # If None, use positional encoding instead of convolution.\n",
    "    clip_embedding:int = None # Clip input value\n",
    "    \n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1.0 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "\n",
    "    # 配列中の偶数インデックスにはsinを適用; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # 配列中の奇数インデックスにはcosを適用; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "class WaveEncoder(tf.keras.Model):\n",
    "    def __init__(self, config: Union[EncoderConfig,dict]):\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(config, dict):\n",
    "            config = EncoderConfig(**config)\n",
    "        self.config = config\n",
    "        \n",
    "        embedding_layers = []\n",
    "        if config.hidden_mask_prob > 0:\n",
    "            embedding_layers.append(tf.keras.layers.Dropout(config.hidden_mask_prob))\n",
    "        if config.embedding_kernel is not None:\n",
    "            embedding_layers.append(\n",
    "                tf.keras.layers.Conv1D(config.hidden_size, kernel_size=config.embedding_kernel)\n",
    "            )\n",
    "        \n",
    "        self.embedding = tf.keras.Sequential(embedding_layers)\n",
    "        self.positional_encoding = positional_encoding(config.seq_size, config.hidden_size)\n",
    "\n",
    "        _config = utils.get_default_config()\n",
    "        _config.update({\n",
    "            'num_attention_heads': config.num_attention_heads, \n",
    "            'num_hidden_layers': config.num_hidden_layers,\n",
    "            'intermediate_size': config.intermediate_size,\n",
    "            'hidden_size': config.hidden_size\n",
    "        })\n",
    "        self.encoder = EncoderStack(_config)\n",
    "\n",
    "    def call(self, xs, mask=None, training=False):\n",
    "        assert mask is not None\n",
    "        embedding_output = self.embedding(xs, training=training)\n",
    "        sequence_output = self.encoder(embedding_output, mask, training=training)\n",
    "        return sequence_output\n",
    "\n",
    "class PretrainingModel(tf.keras.Model):\n",
    "    def __init__(self, n_ch: int, config:EncoderConfig = EncoderConfig()):\n",
    "        super().__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "            WaveEncoder(config),\n",
    "            tf.keras.layers.Conv1D(64, kernel_size=21, padding='SAME'),\n",
    "            tf.keras.layers.Conv1D(64, kernel_size=11, padding='SAME'),\n",
    "            tf.keras.layers.Conv1D(n_ch, kernel_size=3, padding='SAME'),\n",
    "            tf.keras.layers.Activation('tanh'),\n",
    "        ])\n",
    "\n",
    "    def call(self, xs, training=False, mask=None):\n",
    "        if mask is None:\n",
    "            mask = tf.ones_like(xs)\n",
    "        return self.model(xs, mask=mask, training=training)\n",
    "\n",
    "    def make_random_mask(self, xs):\n",
    "        prob = tf.random.uniform((xs.shape[0], xs.shape[1], 1), 0.0, 1.0)\n",
    "        drop_mask = prob <= 0.1\n",
    "        random_mask = tf.math.logical_and(prob > 0.1, prob < 0.9)\n",
    "        estimate_mask = prob > 0.1\n",
    "\n",
    "        xs = tf.where(drop_mask, xs, 0.0)\n",
    "        xs = tf.where(random_mask, xs, tf.random.uniform(tf.shape(xs), 0.0, 1.0))\n",
    "        return xs, estimate_mask[..., 0]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        xs = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            xs_input, mask = self.make_random_mask(xs)\n",
    "            ys_pred = self(xs_input, mask=mask, training=True)  # Forward pass\n",
    "            # Compute the loss value.\n",
    "            # The loss function is configured in `compile()`.\n",
    "            \n",
    "            ys = tf.boolean_mask(xs, mask)\n",
    "            ys_pred = tf.boolean_mask(ys_pred, mask)\n",
    "            loss = self.compiled_loss(\n",
    "                ys, ys_pred,\n",
    "                regularization_losses=self.losses,\n",
    "            )\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.compiled_metrics.update_state(ys, ys_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}   \n",
    "\n",
    "    def test_step(self, data):\n",
    "        xs = data\n",
    "        xs_input, mask = self.make_random_mask(xs)\n",
    "        # Compute predictions\n",
    "        ys_pred = self(xs_input, mask=mask, training=False)\n",
    "        # print(xs.shape, mask.shape, ys_pred.shape)\n",
    "        # Updates the metrics tracking the loss\n",
    "        ys = tf.boolean_mask(xs, mask)\n",
    "        ys_pred = tf.boolean_mask(ys_pred, mask)\n",
    "        self.compiled_loss(ys, ys_pred, regularization_losses=self.losses)\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(ys, ys_pred)\n",
    "        # Return a dict mapping metric names to current value.\n",
    "        # Note that it will include the loss (tracked in self.metrics).\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/2000\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "in user code:\n\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-56-7e2ea005bcbb>:101 train_step\n        xs_input, mask = self.make_random_mask(xs)\n    <ipython-input-56-7e2ea005bcbb>:89 make_random_mask\n        prob = tf.random.uniform((xs.shape[0], xs.shape[1], 1), 0.0, 1.0)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py:289 random_uniform\n        shape = tensor_util.shape_tensor(shape)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:1035 shape_tensor\n        return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:265 constant\n        allow_broadcast=True)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:553 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, None, 1). Consider casting elements to a supported type.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-76e508bca9c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0msave_weights_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             )\n\u001b[0;32m     74\u001b[0m         ]\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-56-7e2ea005bcbb>:101 train_step\n        xs_input, mask = self.make_random_mask(xs)\n    <ipython-input-56-7e2ea005bcbb>:89 make_random_mask\n        prob = tf.random.uniform((xs.shape[0], xs.shape[1], 1), 0.0, 1.0)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\random_ops.py:289 random_uniform\n        shape = tensor_util.shape_tensor(shape)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:1035 shape_tensor\n        return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163 wrapped\n        return func(*args, **kwargs)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:265 constant\n        allow_broadcast=True)\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    c:\\users\\kazbi\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py:553 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class 'tuple'> to Tensor. Contents: (None, None, 1). Consider casting elements to a supported type.\n"
     ]
    }
   ],
   "source": [
    "#pretrain\n",
    "import IPython \n",
    "from collections import defaultdict \n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "import imageio\n",
    "from IPython.display import Image, display_png, clear_output\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "\n",
    "epochs = 2000\n",
    "max_attention_heads = 8\n",
    "\n",
    "model = PretrainingModel(\n",
    "    2, \n",
    "    {\n",
    "        # 'attention_type': 'simulated_sparse',\n",
    "        'embedding_kernel':1, \n",
    "        'seq_size': 16000,\n",
    "        'intermediate_size': 1024, \n",
    "        'hidden_size': max_attention_heads * 15 ,\n",
    "        \"num_attention_heads\": max_attention_heads,\n",
    "        \"num_hidden_layers\": 3,\n",
    "    }\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.99)\n",
    "\n",
    "checkpoint_name = f'{task}_pretrain_fold{fold_index}'\n",
    "cur_model_dir = model_dir / checkpoint_name\n",
    "cur_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def get_train_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        lambda wave, mask, label: wave\n",
    "    )\n",
    "tf.config.run_functions_eagerly(False)\n",
    "plot_callback = PlotCallback(n_step=3)\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=optimizer,\n",
    "        metrics=tf.keras.losses.MeanAbsoluteError(),\n",
    "    )\n",
    "    \n",
    "    mode = 'min'\n",
    "    model.fit(\n",
    "        get_train_dataset(train_dataset),\n",
    "        batch_size=batch_size, epochs=2000, shuffle=True,\n",
    "        validation_data=get_train_dataset(test_dataset),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                patience=50, \n",
    "                monitor='val_loss',\n",
    "                #monitor='val_balanced_sparse_categorical_accuracy',\n",
    "                mode=mode\n",
    "            ),\n",
    "            plot_callback,\n",
    "            tf.keras.callbacks.TerminateOnNaN(),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                str(cur_model_dir / (checkpoint_name + '.model')),\n",
    "                monitor='val_loss',\n",
    "                #monitor='val_balanced_sparse_categorical_accuracy', \n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=mode, \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "plot_callback.fig.tight_layout()\n",
    "plot_callback.fig.savefig(str(cur_model_dir / (checkpoint_name + '.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.get_default_config()\n",
    "next(get_train_dataset(train_dataset).as_numpy_iterator()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "    155/Unknown - 5s 35ms/step - loss: 4.7111 - balanced_sparse_categorical_accuracy: 0.2301"
     ]
    }
   ],
   "source": [
    "import IPython \n",
    "from collections import defaultdict \n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "import imageio\n",
    "from IPython.display import Image, display_png, clear_output\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "epochs = 2000\n",
    "base_net = create_model()\n",
    "base_net.add(tf.keras.layers.Activation('softmax'))\n",
    "model = WavegramCNN(base_net)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5, beta_2=0.99)\n",
    "\n",
    "checkpoint_name = f'{task}_cnn_{model_type}_fold{fold_index}'\n",
    "cur_model_dir = model_dir / checkpoint_name\n",
    "cur_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "plot_callback = PlotCallback(n_step=3)\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction=tf.keras.losses.Reduction.SUM),\n",
    "        optimizer=optimizer,\n",
    "        metrics=BalancedSparseCategoricalAccuracy(),\n",
    "#         options=tf.distribute.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "    )\n",
    "    \n",
    "    mode = 'min'\n",
    "    model.fit(\n",
    "        train_dataset.map(lambda *vars_list: (vars_list[0], vars_list[-1])),\n",
    "        batch_size=batch_size, epochs=2000, shuffle=True,\n",
    "        validation_data=test_dataset.map(lambda *vars_list: (vars_list[0], vars_list[-1])),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                patience=50, \n",
    "                monitor='val_loss',\n",
    "                #monitor='val_balanced_sparse_categorical_accuracy',\n",
    "                mode=mode\n",
    "            ),\n",
    "            plot_callback,\n",
    "            tf.keras.callbacks.TerminateOnNaN(),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                str(cur_model_dir / (checkpoint_name + '.model')),\n",
    "                monitor='val_loss',\n",
    "                #monitor='val_balanced_sparse_categorical_accuracy', \n",
    "                save_best_only=True,\n",
    "                save_weights_only=True,\n",
    "                mode=mode, \n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "plot_callback.fig.tight_layout()\n",
    "plot_callback.fig.savefig(str(cur_model_dir / (checkpoint_name + '.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_data = []\n",
    "model = create_model()\n",
    "model.load_weights(str(cur_model_dir / (checkpoint_name + '.model')))\n",
    "model.compile()\n",
    "\n",
    "results = {}\n",
    "for target_name, dataset in zip(['train', 'test'], (train_dataset, test_dataset)):\n",
    "    pred_logits = model.predict(\n",
    "        dataset.map(lambda audios, norm_audios, labels: (audios, labels))\n",
    "    )\n",
    "    pred_labels = tf.argmax(tf.nn.softmax(pred_logits, axis=1), axis=1)\n",
    "    \n",
    "    audio = []\n",
    "    truth_labels = []\n",
    "    for batch in dataset:\n",
    "        audio.extend(batch[1].numpy())\n",
    "        truth_labels.extend(batch[2].numpy())\n",
    "    metric = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "        tf.convert_to_tensor(np.array(truth_labels).astype(np.int32)), \n",
    "        tf.convert_to_tensor(pred_logits.astype(np.float32))\n",
    "    )).numpy()\n",
    "\n",
    "    truth_labels = np.array(sound_events).take(truth_labels)\n",
    "    pred_labels = np.array(sound_events).take(pred_labels)\n",
    "    \n",
    "    agg_df = pd.crosstab(\n",
    "        pd.Series(truth_labels, name='Truth'),\n",
    "        pd.Series(pred_labels, name='Prediction'),\n",
    "    )\n",
    "    agg_df = agg_df.reindex(columns=sound_events, index=sound_events, fill_value=0)\n",
    "    display(target_name)\n",
    "    display(agg_df)\n",
    "    \n",
    "    accuracy = {}\n",
    "    for name in sound_events:\n",
    "        mask = truth_labels == name\n",
    "        accuracy[name] = accuracy_score(truth_labels[mask], pred_labels[mask]) \n",
    "    accuracy['Metric'] = metric\n",
    "    accuracy_data.append(pd.Series(accuracy, name=target_name))\n",
    "    \n",
    "    results[target_name] = {\n",
    "        'Audio': None if save_without_train and target_name == 'train' else audio,\n",
    "        'Prediction': pd.DataFrame({\n",
    "            'Prediction': pred_labels,\n",
    "            'Truth': truth_labels,\n",
    "        }),\n",
    "        'Agg': agg_df,\n",
    "        'Accuracy': accuracy_data,\n",
    "    }\n",
    "accuracy_df = pd.DataFrame(accuracy_data)\n",
    "accuracy_df['Mean'] = accuracy_df.mean(axis=1)\n",
    "display(accuracy_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as pickle\n",
    "with open(cur_model_dir / f'result_metric.pickle', 'wb+') as fp:\n",
    "    pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_without_train and 'train' == 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}